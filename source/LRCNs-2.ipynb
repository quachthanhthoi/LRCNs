{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 if using cudnn on GPU else using CPU\n",
    "CUDNN_GPU = 0\n",
    "model_path = 'model.xlsx'\n",
    "data_path = '../data/LRCN_dataset.npy'\n",
    "model_saver = 'model_saver/'\n",
    "n_classes = 4\n",
    "n_input = [224,224,3]\n",
    "n_output = [4]\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order</th>\n",
       "      <th>Layer</th>\n",
       "      <th>Filter</th>\n",
       "      <th>kernel</th>\n",
       "      <th>Stride</th>\n",
       "      <th>Padding</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Input</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>224x224x3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Conv</td>\n",
       "      <td>96.0</td>\n",
       "      <td>7x7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>VALID</td>\n",
       "      <td>110x110x96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MaxPooling</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3x3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55x55x96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Conv</td>\n",
       "      <td>256.0</td>\n",
       "      <td>5x5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>VALID</td>\n",
       "      <td>26x26x256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MaxPooling</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3x3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13x13x256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Conv</td>\n",
       "      <td>384.0</td>\n",
       "      <td>3x3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SAME</td>\n",
       "      <td>13x13x384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Conv</td>\n",
       "      <td>384.0</td>\n",
       "      <td>3x3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SAME</td>\n",
       "      <td>13x13x384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Conv</td>\n",
       "      <td>256.0</td>\n",
       "      <td>3x3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SAME</td>\n",
       "      <td>13x13x256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>MaxPooling</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3x3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6x6x256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Flatten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9216x1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Fc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4096x1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Fc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4096x1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Fc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4x1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Softmax</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4x1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Order       Layer  Filter kernel  Stride Padding      Output\n",
       "0       0       Input     NaN    NaN     NaN     NaN   224x224x3\n",
       "1       1        Conv    96.0    7x7     2.0   VALID  110x110x96\n",
       "2       2  MaxPooling     NaN    3x3     2.0     NaN    55x55x96\n",
       "3       3        Conv   256.0    5x5     2.0   VALID   26x26x256\n",
       "4       4  MaxPooling     NaN    3x3     2.0     NaN   13x13x256\n",
       "5       5        Conv   384.0    3x3     1.0    SAME   13x13x384\n",
       "6       6        Conv   384.0    3x3     1.0    SAME   13x13x384\n",
       "7       7        Conv   256.0    3x3     1.0    SAME   13x13x256\n",
       "8       8  MaxPooling     NaN    3x3     2.0     NaN     6x6x256\n",
       "9       9     Flatten     NaN    NaN     NaN     NaN      9216x1\n",
       "10     10          Fc     NaN    NaN     NaN     NaN      4096x1\n",
       "11     11          Fc     NaN    NaN     NaN     NaN      4096x1\n",
       "12     12          Fc     NaN    NaN     NaN     NaN         4x1\n",
       "13     13     Softmax     NaN    NaN     NaN     NaN         4x1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(data_path).item()\n",
    "#data['X_train'] = data['X_train']/255\n",
    "my_model = pd.read_excel(model_path)\n",
    "num_layer = my_model.values.shape[0]\n",
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_op(model, y):\n",
    "    model = tf.Print(model,[model],summarize=20,message=\"model\")\n",
    "    y = tf.Print(y,[y],summarize=20,message=\"y____\")\n",
    "    \n",
    "#     loss =  tf.reduce_sum(tf.square(model - y),[1])\n",
    "#     return tf.reduce_mean(loss)\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model,labels= y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_cudnn_on_gpu = True if CUDNN_GPU == 1 else False\n",
    "my_model = pd.read_excel(model_path)\n",
    "\n",
    "### Define some function for making layers\n",
    "def make_input(input_, out_shape, name):\n",
    "    #return tf.reshape(input_, [1,out_shape[0],out_shape[1],out_shape[2]], name=name)\n",
    "    result = input_\n",
    "    tf.summary.histogram(name, result)\n",
    "    return result\n",
    "\n",
    "def make_conv(input_, in_channel, out_channel, filter_, strides, padding_,  name):\n",
    "    out_channel = int(out_channel)\n",
    "    strides = int(strides)\n",
    "    filter_ = list(map(int,filter_.split('x')))\n",
    "    filter_ = tf.Variable(tf.random_normal([filter_[0],filter_[1],in_channel,out_channel],stddev=0.01), trainable=True, name=name+'_filter')\n",
    "    # conv\n",
    "    result = tf.nn.conv2d(input=input_, \n",
    "                     filter=filter_,\n",
    "                     strides=[1, strides, strides, 1],\n",
    "                     padding=padding_,\n",
    "                     use_cudnn_on_gpu=use_cudnn_on_gpu,\n",
    "                     name=name)\n",
    "    # add bias\n",
    "    bias = tf.Variable(tf.random_normal([out_channel]))\n",
    "    result = tf.nn.bias_add(result, bias)\n",
    "    # relu\n",
    "    result = tf.nn.relu(result)\n",
    "\n",
    "    tf.summary.histogram(name, result)\n",
    "    return result\n",
    "\n",
    "def make_maxpool(input_, in_channel, filter_, strides, name):\n",
    "    strides = int(strides)\n",
    "    filter_ = list(map(int,filter_.split('x')))\n",
    "    result = tf.nn.max_pool(value=input_,\n",
    "                         ksize=[1, filter_[0], filter_[1], 1],\n",
    "                         strides=[1, strides, strides, 1],\n",
    "                         padding='SAME',\n",
    "                         name=name)\n",
    "    return result\n",
    "\n",
    "def make_flatten(input_, name):\n",
    "    #input_ = tf.Print(input_,[input_],summarize=20,message='flatten')\n",
    "    return tf.contrib.layers.flatten(inputs=input_, scope=name)\n",
    "\n",
    "def make_fc(input_, out_shape, name):\n",
    "    result = tf.contrib.layers.fully_connected(inputs=input_,\n",
    "                                                activation_fn=tf.nn.sigmoid,\n",
    "                                                num_outputs=int(out_shape[0]),\n",
    "                                                scope=name)\n",
    "    #result = tf.layers.dense(input_, units=int(out_shape[0]), name=name)\n",
    "    tf.summary.histogram(name, result)\n",
    "    return result\n",
    "\n",
    "def make_dropout(input_, name):\n",
    "    return tf.nn.dropout(input_, 0.75)\n",
    "\n",
    "def make_reshape(input_, out_shape, name):\n",
    "    result = tf.reshape(tensor=input_,\n",
    "                     shape=[tf.shape(input_)[0],out_shape[0],out_shape[1],out_shape[2]],\n",
    "                     name=name)\n",
    "    tf.summary.histogram(name, result)\n",
    "    return result\n",
    "\n",
    "def make_sigmoid(input_, name):\n",
    "    return tf.nn.sigmoid(input_, name=name)\n",
    "\n",
    "def make_softmax(input_, name):\n",
    "    #input_ = tf.Print(input_,[input_],summarize=20,message='Before softmax')\n",
    "    return tf.nn.softmax(input_, name=name)\n",
    "    #return input_\n",
    "\n",
    "### Generate the model base on the model file\n",
    "#output.append(input)\n",
    "\n",
    "layer_match = {\n",
    "        'Input':      lambda input_, params, _:       make_input(input_, params[-1], 'input_image'),\n",
    "        'Conv':       lambda input_, params, channel: make_conv(input_, channel, params[2], params[3], params[4], str(params[5]), 'layer_'+str(params[0])),\n",
    "        'MaxPooling': lambda input_, params, channel: make_maxpool(input_, channel, params[3], params[4], 'layer_'+str(params[0])),\n",
    "        'Flatten':    lambda input_, params, _:       make_flatten(input_, 'layer_'+str(params[0])),\n",
    "        'Fc':         lambda input_, params, _:       make_fc(input_, params[-1], 'layer_'+str(params[0])),\n",
    "        'Reshape':    lambda input_, params, _:       make_reshape(input_, params[-1], 'layer_'+str(params[0])),\n",
    "        'Dropout':    lambda input_, params, _:       make_dropout(input_, 'layer_'+str(params[0])),\n",
    "        'Sigmoid':    lambda input_, params, _:       make_sigmoid(input_, 'layer_'+str(params[0])),\n",
    "        'Softmax':    lambda input_, params, _:       make_softmax(input_, 'layer_'+str(params[0]))\n",
    "    }\n",
    "prev_channel = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1742 333\n"
     ]
    }
   ],
   "source": [
    "train_len = data['X_train'].shape[0]\n",
    "test_len = data['X_test'].shape[0]\n",
    "print(train_len, test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1742, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['y_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# the placeholders will be used for the feed_dict param of tf.Session.run()\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "X = tf.placeholder(tf.float32, [None, n_input[0], n_input[1], n_input[2]], name='X_train')\n",
    "y = tf.placeholder(tf.float32, [None, n_output[0]], name='y_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(input, is_training=True):\n",
    "    # model\n",
    "    #cnn_model = cnn_model(X, model_path, is_training)\n",
    "    input2 = make_input(input,[224,224,3],'input')\n",
    "    conv1 = make_conv(input2,3,96,'7x7',2,'VALID','conv1')\n",
    "    pool1 = make_maxpool(conv1,96,'3x3',2,'pool1')\n",
    "    conv2 = make_conv(pool1,96,256,'5x5',2,'VALID','conv2')\n",
    "    pool2 = make_maxpool(conv2,256,'3x3',2,'pool2')\n",
    "    conv3 = make_conv(pool2,256,384,'3x3',1,'SAME','conv3')\n",
    "    conv4 = make_conv(conv3,384,384,'3x3',1,'SAME','conv4')\n",
    "    conv5 = make_conv(conv4,384,256,'3x3',1,'SAME','conv5')\n",
    "    pool5 = make_maxpool(conv5,256,'3x3',2,'pool5')\n",
    "    flatten = make_flatten(pool5,'flatten')\n",
    "    fc1 = make_fc(flatten,[4096,1],'fc1')\n",
    "    fc2 = make_fc(fc1,[4096,1],'fc2')\n",
    "    fc3 = make_fc(fc2,[4,1],'fc3')\n",
    "    softmax = make_softmax(fc3,'softmax')\n",
    "    \n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn_model1 = cnn_model(X)\n",
    "\n",
    "# learning rate\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "# learning_rate = tf.train.exponential_decay(\n",
    "#                         0.001,  # Base learning rate.\n",
    "#                         global_step,  # Current index into the dataset.\n",
    "#                         train_len,  # Decay step.\n",
    "#                         0.95,  # Decay rate.\n",
    "#                         staircase=True,\n",
    "#                         name='learning_rate')\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "\n",
    "# loss \n",
    "loss_op = loss_op(cnn_model1, y);\n",
    "# optimizer \n",
    "learning_rate = tf.Print(learning_rate,[learning_rate],message=\">>>>>>\")\n",
    "optimal = tf.train.AdamOptimizer(learning_rate, name='adam_optimizer')\n",
    "#optimal = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "# increment global_step at each step.\n",
    "train_op = optimal.minimize(loss_op, global_step=global_step, name='optimal_min')\n",
    "\n",
    "# evaluate model\n",
    "correct_prediction = tf.equal(tf.argmax(cnn_model1, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tao summary cua cac monitor de quan sat cac bien\n",
    "tf.summary.scalar('loss_op', loss_op)\n",
    "tf.summary.scalar('learning_rate', learning_rate)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# gop cac summaries vao mot operation\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "# tao doi tuong log writer va ghi vao Tensorboard\n",
    "tf_writer = tf.summary.FileWriter('../checkpoint', graph=tf.get_default_graph())\n",
    "\n",
    "# khoi tao cac variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dict_field_name, batch):\n",
    "    return data[dict_field_name][batch*batch_size:min((batch+1)*batch_size,train_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0001,cost={1.369679213}, training accuracy 0.60000\n",
      "\n",
      "Epoch:0002,cost={1.369844794}, training accuracy 0.60000\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-80381cb4be88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# chay train_op, loss_op, accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_summary_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Write logs at every iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtf_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal_batch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ipykernel_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ipykernel_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ipykernel_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ipykernel_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ipykernel_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ipykernel_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# remove model_saver folder\n",
    "shutil.rmtree(model_saver, ignore_errors=True)\n",
    "\n",
    "# training\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(init, feed_dict={is_training:True})\n",
    "    # Training cycle\n",
    "    epoch = 0;\n",
    "    while True:\n",
    "        total_batch =  train_len // batch_size\n",
    "        #for batch in range(total_batch):\n",
    "        for batch in range(5):\n",
    "            # lay batch tiep theo\n",
    "            batch_input = get_batch('X_train', batch) \n",
    "            batch_label = get_batch('y_train', batch)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # chay train_op, loss_op, accuracy\n",
    "            _, cost, acc, summary = sess.run([train_op, loss_op, accuracy, merged_summary_op], feed_dict={X:batch_input, y:batch_label, is_training:True})\n",
    "            # Write logs at every iteration\n",
    "            tf_writer.add_summary(summary, epoch * total_batch + batch)\n",
    "            #print(\"Epoch:\" + ('%04d,' % (epoch)) + (\"cost={%.9f}, training accuracy %.5f\" % (cost, acc)) + \"\\n\")\n",
    "            \n",
    "            \n",
    "\n",
    "        epoch += 1;\n",
    "        \n",
    "        # hien thi ket qua sau moi epoch\n",
    "        print(\"Epoch:\" + ('%04d,' % (epoch)) + (\"cost={%.9f}, training accuracy %.5f\" % (cost, acc)) + \"\\n\")\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            # Luu tru variables vao disk.\n",
    "            save_path = saver.save(sess, model_saver + 'nn_model_%04d.ckpt'%(epoch))\n",
    "            print(\"Model saved in path: %s \\n\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
